{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWvYBsLey02i",
    "outputId": "b2f623b8-a3c4-49f3-e87b-b875bbd96c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0LBKd9RzRe_",
    "outputId": "6d250ba3-d752-4b49-8aeb-fec013f8d788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rectified-adam in /usr/local/lib/python3.7/dist-packages (0.20.0)\n",
      "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-rectified-adam) (2.8.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-rectified-adam) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "pip install keras-rectified-adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68gRucZcu_sk",
    "outputId": "bbd1f1ab-9557-443e-f859-1f8b255dffa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "IMe4ZKTwrsio"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "LxKbewLchsjB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "import os\n",
    "#from tensorflow.keras import regularizers\n",
    "from keras_radam import RAdam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "-hQdmxQhiV5b"
   },
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "zSe7MYjsSMse"
   },
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "    \n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true + y_pred)\n",
    "    jac = (intersection + 1.) / (union - intersection + 1.)\n",
    "    return K.mean(jac)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "L7aHYyvI63yO"
   },
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(weight0=0.25, weight1=1):\n",
    "  '''\n",
    "  w1 and w2 are the weights for the two classes.\n",
    "  Computes weighted binary crossentropy\n",
    "  Use like so:  model.compile(loss=weighted_binary_crossentropy(), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "  '''\n",
    "\n",
    "  def wbce( y_true, y_pred) :\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1-K.epsilon())\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon())\n",
    "    logloss = -(y_true * K.log(y_pred) * weight1 + (1 - y_true) * K.log(1 - y_pred) * weight0 )\n",
    "    return K.mean( logloss, axis=-1)\n",
    "\n",
    "\n",
    "  return wbce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "O2lQpf9j9hjN"
   },
   "outputs": [],
   "source": [
    "def weighted_bce(y_true, y_pred):\n",
    "  weights = (y_true * 59.) + 1.\n",
    "  bce = K.binary_crossentropy(y_true, y_pred)\n",
    "  weighted_bce = K.mean(bce * weights)\n",
    "  return weighted_bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "0qUB1WGt8AVS"
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import Sequence\n",
    "import cv2\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    def __init__(self, batch_size, dim, input_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.shuffle = False\n",
    "        self.n_channels = 1\n",
    "        self.on_epoch_end()\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return len(self.input_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        i = idx * self.batch_size  \n",
    "\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        #batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X = self._generate_X(batch_input_img_paths)\n",
    "        y = self._generate_y(batch_input_img_paths)\n",
    "        \n",
    "        return X,y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        pass\n",
    "\n",
    "    # Normalizes values of a matrix between -1 and 1\n",
    "    def myNormalization(self, data):\n",
    "        max_val = np.max(data)\n",
    "        min_val = np.min(data)\n",
    "        \n",
    "        return  2 * (data[:,:] - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "\n",
    "    def _load_input_image(self, image_path):\n",
    "        'Load image normalized'\n",
    "        img = np.load(image_path)\n",
    "        img = self.myNormalization(img)\n",
    "        img = cv2.resize(img.astype('float32'), (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "        return img\n",
    "\n",
    "    def _load_target_image(self, image_path):\n",
    "        'Load target image'\n",
    "        img = np.load(image_path)\n",
    "        img = cv2.resize(img.astype('float32'), (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "        img = np.uint8(img)\n",
    "        \n",
    "        #img = np.reshape(img, (300,300,1))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _generate_X(self, batch_input_img_paths):\n",
    "        'Generates data containing batch_size images'\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, path in enumerate(batch_input_img_paths):\n",
    "            # Store sample\n",
    "            aux =  self._load_input_image(path)\n",
    "            X[i,] = np.reshape( aux,  (224,224,1) )\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def _generate_y(self, batch_input_img_paths):\n",
    "        'Generates data containing batch_size masks'\n",
    "        y =  np.empty((self.batch_size, *self.dim))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, path in enumerate(batch_input_img_paths):\n",
    "            # Store sample\n",
    "            y[i,] = np.reshape( self._load_target_image(path.replace('seismic', 'fault').replace('input','target')), (224,224,1))\n",
    "            \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ax7J3jhvWCRW"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "#seed(12345)\n",
    "#import tensorflow as tf\n",
    "#tf.random.set_seed(1234)\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
    "from keras.layers import Input, Activation, Conv2D, MaxPooling2D, UpSampling2D, Dense, Cropping2D, Dropout, Concatenate, BatchNormalization\n",
    "from keras import backend as keras\n",
    "import numpy as np\n",
    "#from tensorflow.keras.utils import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ZtENOCcZ_KFe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wekTW-wIabcP",
    "outputId": "b666b440-790a-4328-f458-d1a555622e2e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "yOqPbKrzMaEV"
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "  return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "mGQBoiBTuwAw"
   },
   "outputs": [],
   "source": [
    "def showHistory(history):\n",
    "  # list all data in history\n",
    "  print(history.history.keys())\n",
    "  fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "  # summarize history for accuracy\n",
    "  plt.plot(history.history['dice_coef'])\n",
    "  plt.plot(history.history['val_dice_coef'])\n",
    "  plt.title('Model accuracy',fontsize=20)\n",
    "  plt.ylabel('dice_coef',fontsize=20)\n",
    "  plt.xlabel('Epoch',fontsize=20)\n",
    "  plt.legend(['train', 'val'], loc='center right',fontsize=20)\n",
    "  plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "  plt.tick_params(axis='both', which='minor', labelsize=18)\n",
    "  plt.show()\n",
    "\n",
    "  # summarize history for loss\n",
    "  fig = plt.figure(figsize=(10,6))\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model loss',fontsize=20)\n",
    "  plt.ylabel('Loss',fontsize=20)\n",
    "  plt.xlabel('Epoch',fontsize=20)\n",
    "  plt.legend(['train', 'val'], loc='center right',fontsize=20)\n",
    "  plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "  plt.tick_params(axis='both', which='minor', labelsize=18)\n",
    "  plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "AoNbzLa0ABL_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def unet (pretrained_weights = None,bn = False, do= False, input_size = (None,None,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1) if bn else conv1\n",
    "    drop1 = Dropout(0.4)(conv1) if do else conv1\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2) if bn else conv2\n",
    "    drop2 = Dropout(0.4)(conv2) if do else conv2\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3) if bn else conv3\n",
    "    drop3 = Dropout(0.4)(conv3) if do else conv3\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4) if bn else conv4\n",
    "    drop4 = Dropout(0.4)(conv4) if do else conv4\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.4)(conv5)if do else conv5\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    drop6 = Dropout(0.4)(merge6)if do else merge6\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(drop6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    drop7 = Dropout(0.4)(merge7)if do else merge7\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(drop7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    drop8 = Dropout(0.4)(merge8)if do else merge8\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(drop8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    drop9 = Dropout(0.4)(merge9)if do else merge9\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(drop9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "MzsswOuTifjl",
    "outputId": "6d46ae49-a93b-45ae-fa06-1eb65a576427"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'root_dir = \"/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/inpt_ampl-targt_3DDE\"\\n\\n# Creating partitions of the data after shuffeling\\ncurrentCls = \\'/target-2021-10-14\\'\\nsrc = root_dir+currentCls # Folder to copy images from input first\\nallFileNames = os.listdir(src)\\n#Split an array into multiple sub-arrays as views into ndarray.\\n#train 70%\\n#val 15%\\n#test 15%\\nimport time\\nseed = np.random.randint(0, 100000)  \\nnp.random.seed(seed) \\nnp.random.shuffle(allFileNames)\\ntrain_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\\n                                                          [int(len(allFileNames)*0.8), int(len(allFileNames)*0.90)])\\n\\n\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''root_dir = \"/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/inpt_ampl-targt_3DDE\"\n",
    "\n",
    "# Creating partitions of the data after shuffeling\n",
    "currentCls = '/target-2021-10-14'\n",
    "src = root_dir+currentCls # Folder to copy images from input first\n",
    "allFileNames = os.listdir(src)\n",
    "#Split an array into multiple sub-arrays as views into ndarray.\n",
    "#train 70%\n",
    "#val 15%\n",
    "#test 15%\n",
    "import time\n",
    "seed = np.random.randint(0, 100000)  \n",
    "np.random.seed(seed) \n",
    "np.random.shuffle(allFileNames)\n",
    "train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                          [int(len(allFileNames)*0.8), int(len(allFileNames)*0.90)])\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "xWwXSobIODQU"
   },
   "outputs": [],
   "source": [
    "root_dir = \"/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/sismica/sismica-interpretada/dados-modelo\"\n",
    "currentCls = '/input'\n",
    "src = root_dir+currentCls\n",
    "\n",
    "train_FileNames = np.load(root_dir+'/names_withfaults-finalaug-train.npy')\n",
    "np.random.shuffle(train_FileNames)\n",
    "val_FileNames = np.load(root_dir+'/names_withfaults-finalaug-val.npy')\n",
    "\n",
    "\n",
    "train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ZLMh1I2cCBGG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "z93L2Of5v3jy"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "def cross_entropy_balanced(y_true, y_pred):\n",
    "    # Note: tf.nn.sigmoid_cross_entropy_with_logits expects y_pred is logits, \n",
    "    # Keras expects probabilities.\n",
    "    # transform y_pred back to logits\n",
    "    _epsilon = _to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n",
    "    y_pred   = tf.clip_by_value(y_pred, _epsilon, 1 - _epsilon)\n",
    "    y_pred   = tf.log(y_pred/ (1 - y_pred))\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    count_neg = tf.reduce_sum(1. - y_true)\n",
    "    count_pos = tf.reduce_sum(y_true)\n",
    "\n",
    "    beta = count_neg / (count_neg + count_pos)\n",
    "\n",
    "    pos_weight = beta / (1 - beta)\n",
    "\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=pos_weight)\n",
    "\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "\n",
    "    return tf.where(tf.equal(count_pos, 0.0), 0.0, cost)\n",
    "\n",
    "\n",
    "def _to_tensor(x, dtype):\n",
    "    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n",
    "    # Arguments\n",
    "    x: An object to be converted (numpy array, list, tensors).\n",
    "    dtype: The destination type.\n",
    "    # Returns\n",
    "    A tensor.\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    if x.dtype != dtype:\n",
    "        x = tf.cast(x, dtype)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "-ZGtUNuo85oQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "def goTrain():\n",
    "   \n",
    "  \n",
    "   #data load first\n",
    "  input_img_paths_train =  train_FileNames\n",
    "  input_img_paths_val = val_FileNames\n",
    "\n",
    "  batch_size = 32\n",
    "  image_size = (224,224,1)\n",
    "\n",
    "  training_generator = DataGenerator(batch_size, image_size, input_img_paths_train)\n",
    "  validation_generator = DataGenerator(batch_size, image_size, input_img_paths_val)\n",
    "  \n",
    "  custom_early_stopping = EarlyStopping(\n",
    "    monitor='val_dice_coef', \n",
    "    patience=15, \n",
    "    min_delta=0.001, \n",
    "    mode='max'\n",
    "  )\n",
    "\n",
    "  #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "  model = Sequential()\n",
    "  model = unet(bn= True, do=False, input_size=((224,224,1)))\n",
    "  \n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(lr=1e-4), loss =  'binary_crossentropy',  metrics=[dice_coef, iou, tf.keras.metrics.Accuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "  # Calculate the weights for each class so that we can balance the data\n",
    "  \n",
    "  model.summary()\n",
    "\n",
    "  # checkpoint\n",
    "  filepath=\"/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/check-unet-inter/fseg-dados_selecionados-dice/unet2D_exec_2022-05-11-EQE+{epoch:02d}.hdf5\"\n",
    "  checkpoint = ModelCheckpoint(filepath, monitor='val_dice_coef', \n",
    "        verbose=1, save_best_only=True, mode='max')\n",
    "  \n",
    "  \n",
    "  print(\"data prepared, ready to train!\")\n",
    "  #model.load_weights('/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/check-unet-inter/fseg-dados_selecionados-dice/unet2D_exec_2022-04-27-EQE+52.hdf5')\n",
    "  #model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = [dice_coef])\n",
    "  \n",
    "  # Fit the model\n",
    "  history_model = model.fit_generator(generator=training_generator,\n",
    "    validation_data=validation_generator,callbacks=[checkpoint, custom_early_stopping], epochs=150)\n",
    "  model.save('/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/check-unet-inter/fseg-dados_selecionados-dice/unet2D_exec_2022-05-11-EQE-150epoc.hdf5')\n",
    "\n",
    "  showHistory(history_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "e3wPRg0v8f6N"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  goTrain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXlD9RVS3k7z",
    "outputId": "18b56318-2ef6-48ed-ea29-3b4b878b17e7"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 224, 224, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 224, 224, 64  640         ['input_4[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 224, 224, 64  36928       ['conv2d_59[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 224, 224, 64  256        ['conv2d_60[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooling2D  (None, 112, 112, 64  0          ['batch_normalization_12[0][0]'] \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 112, 112, 12  73856       ['max_pooling2d_12[0][0]']       \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 112, 112, 12  147584      ['conv2d_61[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 112, 112, 12  512        ['conv2d_62[0][0]']              \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_13 (MaxPooling2D  (None, 56, 56, 128)  0          ['batch_normalization_13[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 56, 56, 256)  295168      ['max_pooling2d_13[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 56, 56, 256)  590080      ['conv2d_63[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 56, 56, 256)  1024       ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_14 (MaxPooling2D  (None, 28, 28, 256)  0          ['batch_normalization_14[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 28, 28, 512)  1180160     ['max_pooling2d_14[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 28, 28, 512)  2359808     ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 28, 28, 512)  2048       ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooling2D  (None, 14, 14, 512)  0          ['batch_normalization_15[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 14, 14, 1024  4719616     ['max_pooling2d_15[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 14, 14, 1024  9438208     ['conv2d_67[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_9 (UpSampling2D)  (None, 28, 28, 1024  0          ['conv2d_68[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 28, 28, 512)  2097664     ['up_sampling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 28, 28, 1024  0           ['batch_normalization_15[0][0]', \n",
      "                                )                                 'conv2d_69[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 28, 28, 512)  4719104     ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 28, 28, 512)  2359808     ['conv2d_70[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_10 (UpSampling2D  (None, 56, 56, 512)  0          ['conv2d_71[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 56, 56, 256)  524544      ['up_sampling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 56, 56, 512)  0           ['batch_normalization_14[0][0]', \n",
      "                                                                  'conv2d_72[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 56, 56, 256)  1179904     ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 56, 56, 256)  590080      ['conv2d_73[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_11 (UpSampling2D  (None, 112, 112, 25  0          ['conv2d_74[0][0]']              \n",
      " )                              6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 112, 112, 12  131200      ['up_sampling2d_11[0][0]']       \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 112, 112, 25  0           ['batch_normalization_13[0][0]', \n",
      "                                6)                                'conv2d_75[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 112, 112, 12  295040      ['concatenate_11[0][0]']         \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 112, 112, 12  147584      ['conv2d_76[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_12 (UpSampling2D  (None, 224, 224, 12  0          ['conv2d_77[0][0]']              \n",
      " )                              8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 224, 224, 64  32832       ['up_sampling2d_12[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 224, 224, 12  0           ['batch_normalization_12[0][0]', \n",
      "                                8)                                'conv2d_78[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 224, 224, 64  73792       ['concatenate_12[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 224, 224, 64  36928       ['conv2d_79[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 224, 224, 2)  1154        ['conv2d_80[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 224, 224, 1)  3           ['conv2d_81[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,035,525\n",
      "Trainable params: 31,033,605\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data prepared, ready to train!\n",
      "Epoch 1/150\n",
      " 28/288 [=>............................] - ETA: 1:22:00 - loss: 0.6938 - dice_coef: 0.2139 - iou: 0.1304 - accuracy: 0.0000e+00 - precision_2: 0.1611 - recall_2: 0.0506"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model-data_tableconstruction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
