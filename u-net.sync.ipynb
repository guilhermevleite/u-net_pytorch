{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80218602",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install albumentations, with qudida\n",
    "# TODO: Find a way to not use albumentations at all\n",
    "!pip install --upgrade --force-reinstall --no-deps qudida==0.0.4\n",
    "!pip install --upgrade --force-reinstall --no-deps albumentations==1.1.0\n",
    "\n",
    "# IF cv2 is not working:\n",
    "!pip uninstall --yes opencv-python-headless==4.5.5.64\n",
    "!pip install opencv-python-headless==4.5.2.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fed527a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62ef33",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84564df3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, m_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.m_transform = m_transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = self.m_transform(mask)\n",
    "\n",
    "        # mask[mask == 255.0] = 1.0\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ec470",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e21022d7",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "        train_dir,\n",
    "        train_maskdir,\n",
    "        val_dir,\n",
    "        val_maskdir,\n",
    "        batch_size,\n",
    "        train_transform,\n",
    "        mask_transform,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "        ):\n",
    "\n",
    "    train_ds = MyDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "        m_transform=mask_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_ds = MyDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=train_transform,\n",
    "        m_transform=mask_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def get_gpu_mem(index=0):\n",
    "    r = torch.cuda.memory_reserved(index)\n",
    "    a = torch.cuda.memory_allocated(index)\n",
    "\n",
    "    free, total = torch.cuda.mem_get_info(index)\n",
    "    free = (free * 8) / (8 * 1000 * 1000 * 1000)\n",
    "    total = (total * 8) / (8 * 1000 * 1000 * 1000)\n",
    "\n",
    "    # return r-a\n",
    "    # return torch.cuda.mem_get_info(index)\n",
    "    return '{:.02f}% Free'.format(free*100/total)\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        state, \n",
    "        filename=\"my_checkpoint.pth.tar\"\n",
    "        ):\n",
    "\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "# Change mode to evaluation and change back to training at the end of this function\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # TODO: What exactly is happening when I call something.to(device)?\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100.0:.2f}\"\n",
    "    )\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "\n",
    "# Change mode back to training mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def save_predictions_as_imgs(\n",
    "        loader,\n",
    "        model,\n",
    "        folder=\"saved_images/\",\n",
    "        device=\"cuda\"\n",
    "        ):\n",
    "\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb05ed",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf759dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        # TODO Figure out what is this doing here. I need its a reference to nn.Module, since we are inheriting that class.\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # First conv\n",
    "            # Conv2d(in_cha, out_cha, kernel, stride, padding)\n",
    "            # When we set stride and padding to one, it is called a SAME CONVOLUTION, the input height and width is the same after the convolution.\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            # There was no BachNorm at the time uNet was published, but it helps, so we are going to use it, and to do that, Conv2d 'bias' argument has to be False.\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Second conv\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    # TODO: What the fuck is this doing? Isn't self.conv just initiated inside __init__?\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        # In the paper, the out channel was 2, we are going to use 1, since all we want is a binary segmentation.\n",
    "        # TODO: Check whether out channel > 1 is necessary only when doing semantic segmentation.\n",
    "        out_channels=1,\n",
    "        # This as the features on every double convolution\n",
    "        features=[64, 128, 256, 512]\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        # We can not use self.downs = [], because it stores the convs and we want do do eval on these.\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Downward path\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # Upward path\n",
    "        # TODO: For a better result we should use Transpose Convolutions\n",
    "        for feature in reversed(features):\n",
    "            # First append is the UP\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    # x2 because of the skip connection.\n",
    "                    feature*2,\n",
    "                    feature,\n",
    "                    # These two will double the height, width of the image\n",
    "                    kernel_size=2,\n",
    "                    stride=2\n",
    "                )\n",
    "            )\n",
    "            # Second append are the TWO CONVS\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "        \n",
    "        # This is the horizontal path between downward and upward\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        # This is the very last conv, from 388,388,64 to 388,388 or as in the paper: 388,388,2. It does not change the size of the image, it only changes the channels.\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        # Simply reversing the list, because of the upward path will use it in inverse order\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        # Step=2 because the upward path has a UP and a DoubleConv, but the skip only applies to the UP part.\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            # Integer division by 2 because, altough we want to skip the DoubleConv, we also want to run through the skip_connections one by one.\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            ''' The INPUT needs to be shaped on a multiple of 16, since it is four down ways. If that is not the case, there will be an error to concatenate because of the MAXPOOL, since them both need same height and width.\n",
    "            One work around this is to check if they are different and resize the X '''\n",
    "            if x.shape != skip_connection.shape:\n",
    "                # Shape has: 0 BATCH_SIZE, 1 N_CHANNELS, 2 HEIGHT, 3 WIDTH. With [2:] we are taking only height and width.\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            # We have 4 dims, 0 BATCH, 1 CHANNEL, 2 HEIGHT, 3 WIDTH. We are concatenating them along the channel dimension\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            # This will do the DoubleConv after we did the UP and concatenated the skip connection\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        \n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3c64c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91217028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "# import utils as Utils\n",
    "\n",
    "from IPython.display import clear_output \n",
    "\n",
    "\n",
    "class Train():\n",
    "\n",
    "    def __init__(self, train_dir, train_maskdir, val_dir, val_maskdir, batch_size, n_epochs, n_workers, learning_rate, img_height, img_width, device, model, loss_fn):\n",
    "        self.train_dir = train_dir\n",
    "        self.train_maskdir = train_maskdir\n",
    "        self.val_dir = val_dir\n",
    "        self.val_maskdir = val_maskdir\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_workers = n_workers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.device = device\n",
    "        self.model = model.to(device=self.device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate\n",
    "                )\n",
    "\n",
    "        # TODO: If needed change Normalize mean to 0.0, and std to 1.0\n",
    "        self.t_transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_height, self.img_width)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (0.5, 0.5, 0.5), \n",
    "                (0.5, 0.5, 0.5)\n",
    "                ),\n",
    "            ])\n",
    "        self.m_transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_height, self.img_width)),\n",
    "            transforms.ToTensor(),\n",
    "            ])\n",
    "\n",
    "        self.train_loader, self.v_loader = get_loaders(\n",
    "                train_dir=self.train_dir,\n",
    "                train_maskdir=self.train_maskdir,\n",
    "                val_dir=self.val_dir,\n",
    "                val_maskdir=self.val_maskdir,\n",
    "                batch_size=self.batch_size,\n",
    "                train_transform=self.t_transform,\n",
    "                mask_transform=self.m_transform,\n",
    "                num_workers=self.n_workers,\n",
    "                pin_memory=True)\n",
    "\n",
    "\n",
    "    def train_one_epoch(self, epoch_index):\n",
    "        running_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            clear_output()\n",
    "            \n",
    "            print('Train One Epoch, train_loader index:', i, len(self.train_loader))\n",
    "            inputs, labels = data\n",
    "\n",
    "            # TODO: This needs to be revised, it was done to fix the input shape into the expected one\n",
    "            # inputs = inputs.permute(0, 3, 1, 2)\n",
    "            # labels = labels.unsqueeze(1)\n",
    "            print('shape:', inputs.shape, labels.shape)\n",
    "\n",
    "            inputs = inputs.float().to(self.device)\n",
    "            labels = labels.float().to(self.device) # Just making sure\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "              outputs = self.model(inputs)\n",
    "              loss = self.loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i%10 == 9:\n",
    "                last_loss = running_loss/1000\n",
    "                print('\\tBatch {} Loss: {}'.format(i+1, last_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # print('1.', Utils.get_gpu_mem())\n",
    "            del(inputs)\n",
    "            del(labels)\n",
    "            torch.cuda.empty_cache()\n",
    "            # print('2.', Utils.get_gpu_mem())\n",
    "\n",
    "        return last_loss\n",
    "\n",
    "\n",
    "    def training(self):\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        epoch_number = 0\n",
    "        best_vloss = 1_000_000.0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            print('Epoch:', epoch+1)\n",
    "\n",
    "            self.model.train(True)\n",
    "            avg_loss = self.train_one_epoch(epoch_number)\n",
    "            self.model.train(False)\n",
    "\n",
    "            # print('Validation for', epoch+1)\n",
    "            running_vloss = 0.0\n",
    "            i = 0\n",
    "            for i,v_data in enumerate(self.v_loader):\n",
    "                v_inputs, v_labels = v_data\n",
    "                print('Input:', v_inputs.shape, 'label:', v_labels.shape)\n",
    "\n",
    "                # print('Before dataloader:', Utils.get_gpu_mem())\n",
    "                v_inputs = v_inputs.permute(0, 3, 1, 2).float().to(self.device)\n",
    "                v_labels = v_labels.unsqueeze(1).float().to(self.device)\n",
    "                # print('After dataloader:', Utils.get_gpu_mem())\n",
    "\n",
    "                # print('Getting predictions')\n",
    "                torch.cuda.empty_cache()\n",
    "                # print('Before model:', Utils.get_gpu_mem())\n",
    "                # with torch.cuda.amp.autocast():\n",
    "                v_outputs = self.model(v_inputs)\n",
    "                # print('After model:', Utils.get_gpu_mem())\n",
    "                # print(v_outputs.shape, v_labels.shape)\n",
    "                v_loss = self.loss_fn(v_outputs, v_labels)                \n",
    "\n",
    "                # print('3.', Utils.get_gpu_mem())\n",
    "                del(v_inputs)\n",
    "                del(v_labels)\n",
    "                torch.cuda.empty_cache()\n",
    "                # print('4.', Utils.get_gpu_mem())\n",
    "\n",
    "                running_vloss += v_loss\n",
    "\n",
    "            # print('Calculating, average loss')\n",
    "            avg_vloss = running_vloss / (i+1)\n",
    "            print('Loss Train: {}\\t Validation: {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "            if avg_vloss < best_vloss:\n",
    "                best_vloss = avg_vloss\n",
    "                model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "                torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "            epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d52b32",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3596e44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train One Epoch, train_loader index: 0 11\n",
      "shape: torch.Size([32, 3, 160, 240]) torch.Size([32, 1, 160, 240])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'amp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5540/1879221530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training U-Net... with'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0munet_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5540/3227864593.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5540/3227864593.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, epoch_index)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m               \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'amp'"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "path_suffix = None\n",
    "\n",
    "if local:\n",
    "    path_suffix = '/home/leite/Drive/'\n",
    "else:\n",
    "    path_suffix = '/content/drive/MyDrive/'\n",
    "\n",
    "print('Suffix:', path_suffix)\n",
    "\n",
    "train_dir = path_suffix + 'db/segmentation/FL5C/train/images/'\n",
    "train_maskdir = path_suffix + 'db/segmentation/FL5C/train/masks/'\n",
    "val_dir = path_suffix + 'db/segmentation/FL5C/val/images/'\n",
    "val_maskdir = path_suffix + 'db/segmentation/FL5C/val/masks/'\n",
    "\n",
    "l_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print('Instantiating U-Net Traning!')\n",
    "# TODO: Define optimizer out here\n",
    "unet_train = Train(\n",
    "        train_dir=train_dir,\n",
    "        train_maskdir=train_maskdir,\n",
    "        val_dir=val_dir,\n",
    "        val_maskdir=val_maskdir,\n",
    "        batch_size=32,\n",
    "        n_epochs=3,\n",
    "        n_workers=2,\n",
    "        learning_rate=1e-4,\n",
    "        img_height=160,\n",
    "        img_width=240,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model=UNET(in_channels=3, out_channels=1),\n",
    "        loss_fn=l_func\n",
    "        )\n",
    "\n",
    "print('Training U-Net... with', unet_train.device)\n",
    "unet_train.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb2bed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Done Training')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
