{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80218602",
   "metadata": {
    "id": "80218602"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771cbbc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "771cbbc9",
    "outputId": "ace44c70-dccd-4923-b501-83c4d8fb9e2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting opencv-python-headless==4.5.2.52\n",
      "  Downloading opencv_python_headless-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.2 MB 1.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.5.2.52) (1.21.6)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.5.2.52\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "cv2"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install albumentations, with qudida\n",
    "# TODO: Find a way to not use albumentations at all\n",
    "# !pip install --upgrade --force-reinstall --no-deps qudida==0.0.4\n",
    "# !pip install --upgrade --force-reinstall --no-deps albumentations==1.1.0\n",
    "\n",
    "\n",
    "# IF cv2 is not working:\n",
    "import cv2\n",
    "if cv2.__version__ != '4.5.2':\n",
    "  !pip uninstall --yes opencv-python-headless==4.5.5.64\n",
    "  !pip install opencv-python-headless==4.5.2.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7af6788",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7af6788",
    "outputId": "897990e4-a75d-485a-ccf4-1840ab31398c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed527a",
   "metadata": {
    "id": "1fed527a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62ef33",
   "metadata": {
    "id": "6d62ef33"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84564df3",
   "metadata": {
    "id": "84564df3"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, m_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.m_transform = m_transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    # My adaptation\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "\n",
    "        # print('Getting:\\n', img_path, '\\n', mask_path)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = self.m_transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    # Video method\n",
    "    # def __getitem__(self, index):\n",
    "    #   img_path = os.path.join(self.image_dir, self.images[index])\n",
    "    #   mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "\n",
    "    #   image = np.array(Image.open(img_path).convert('RGB'))\n",
    "    #   mask = np.array(Image.open(mask_path).convert('L'), dtype=np.float32)\n",
    "\n",
    "    #   mask[mask == 255.0] = 1.\n",
    "    #   image = self.transform(image)\n",
    "    #   mask = self.m_transform(mask)\n",
    "\n",
    "    #   return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ec470",
   "metadata": {
    "id": "085ec470"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R6NyDvcH92kx",
   "metadata": {
    "cellView": "code",
    "id": "R6NyDvcH92kx"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "if not os.path.isdir('saved_images'):\n",
    "  !mkdir saved_images\n",
    "  \n",
    "def get_loaders(\n",
    "        train_dir,\n",
    "        train_maskdir,\n",
    "        val_dir,\n",
    "        val_maskdir,\n",
    "        batch_size,\n",
    "        train_transform,\n",
    "        mask_transform,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "        ):\n",
    "\n",
    "    train_ds = MyDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "        m_transform=mask_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_ds = MyDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=train_transform,\n",
    "        m_transform=mask_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def get_gpu_mem(index=0):\n",
    "    r = torch.cuda.memory_reserved(index)\n",
    "    a = torch.cuda.memory_allocated(index)\n",
    "\n",
    "    free, total = torch.cuda.mem_get_info(index)\n",
    "    free = (free * 8) / (8 * 1000 * 1000 * 1000)\n",
    "    total = (total * 8) / (8 * 1000 * 1000 * 1000)\n",
    "\n",
    "    # return r-a\n",
    "    # return torch.cuda.mem_get_info(index)\n",
    "    return '{:.02f}% Free'.format(free*100/total)\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        state, \n",
    "        filename=\"my_checkpoint.pth.tar\"\n",
    "        ):\n",
    "\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "# Change mode to evaluation and change back to training at the end of this function\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # TODO: What exactly is happening when I call something.to(device)?\n",
    "            x = x.to(device)\n",
    "            # y = y.to(device).unsqueeze(1)\n",
    "            y = y.to(device)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "    \n",
    "    dice = dice_score.item()/len(loader)\n",
    "    acc = num_correct / num_pixels * 100\n",
    "\n",
    "    # print('Batch results:')\n",
    "    # print('\\tDice score:', dice_score.item()/len(loader))\n",
    "    # print('\\tAccuracy: {:.2f}'.format(num_correct / num_pixels * 100)) \n",
    "    #     f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "    # )\n",
    "    # print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "\n",
    "# Change mode back to training mode\n",
    "    model.train()\n",
    "\n",
    "    return dice, acc\n",
    "\n",
    "\n",
    "def save_predictions_as_imgs(\n",
    "        loader,\n",
    "        model,\n",
    "        folder=\"saved_images/\",\n",
    "        device=\"cuda\"\n",
    "        ):\n",
    "\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            # preds = model(x)\n",
    "            print('preds', torch.max(preds))\n",
    "            preds = preds * 255\n",
    "            print('preds', torch.max(preds))\n",
    "            # preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y, f\"{folder}{idx}.png\")\n",
    "        render_img(preds.to('cpu'))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def plot_predictions(loader):\n",
    "    _, ax = plt.subplots(4, 2)\n",
    "    ax[0, 0].set_title(\"Input\")\n",
    "    ax[0, 1].set_title(\"Target\")\n",
    "\n",
    "    l_iter = iter(loader)\n",
    "    img, label = l_iter.next()\n",
    "\n",
    "    img = torch.permute(img, (0, 2, 3, 1))\n",
    "    label = label.squeeze(1)\n",
    "\n",
    "    print('Length', len(img[0])-1)\n",
    "    for i in range(len(img[0])-1):\n",
    "      if i > 3:\n",
    "        break\n",
    "\n",
    "      print(i)\n",
    "      print(torch.max(img[i]))\n",
    "      img[i] = img[i] * 255\n",
    "      print(torch.max(img[i]))\n",
    "\n",
    "      print(img[i].shape, label[i].shape)\n",
    "      ax[i, 0].imshow(img[i])\n",
    "      ax[i, 1].imshow(label[i], cmap='gray')\n",
    "      ax[i, 0].axis(\"off\")\n",
    "      ax[i, 1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_lists(list_one, list_two=None):\n",
    "  if list_two == None:\n",
    "    plt.plot(list_one)\n",
    "    plt.show()\n",
    "    \n",
    "  else:\n",
    "    plt.plot(list_one, list_two)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def render_img(img):\n",
    "    # img[0] = img[0] * 255\n",
    "    img = img[0].permute((1, 2, 0))\n",
    "    img = img.numpy().astype(int)\n",
    "    print(type(img), img.shape, img.max())\n",
    "    cv2.imwrite('preview.png', img)\n",
    "    # img = img[0].permute((1, 2, 0))\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.imshow(img)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb05ed",
   "metadata": {
    "id": "99cb05ed"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf759dfa",
   "metadata": {
    "id": "cf759dfa"
   },
   "outputs": [],
   "source": [
    "# DoubleConv are the two blue arrows in the u_net diagram, horizontally\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        # TODO Figure out what is this doing here. I need its a reference to nn.Module, since we are inheriting that class.\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # First conv\n",
    "            # Conv2d(in_cha, out_cha, kernel, stride, padding)\n",
    "            # When we set stride and padding to one, it is called a SAME CONVOLUTION, the input height and width is the same after the convolution.\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            # There was no BachNorm at the time uNet was published, but it helps, so we are going to use it, and to do that, Conv2d 'bias' argument has to be False.\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Second conv\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    # TODO: What the fuck is this doing? Isn't self.conv just initiated inside __init__?\n",
    "    # This is the blue arrow in the u_net diagram\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        # In the paper, the out channel was 2, we are going to use 1, since all we want is a binary segmentation.\n",
    "        # TODO: Check whether out channel > 1 is necessary only when doing semantic segmentation.\n",
    "        out_channels=1,\n",
    "        # This as the features on every double convolution\n",
    "        features=[64, 128, 256, 512]\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        # We can not use self.downs = [], because it stores the convs and we want do do eval on these.\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Downward path, red arrows in the u_net diagram\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # Upward path, green arrows in the u_net diagram\n",
    "        # TODO: For a better result we should use Transpose Convolutions\n",
    "        for feature in reversed(features):\n",
    "            # First append is the UP\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    # x2 because of the skip connection.\n",
    "                    feature*2,\n",
    "                    # This is the output\n",
    "                    feature,\n",
    "                    # These two will double the height, width of the image\n",
    "                    kernel_size=2,\n",
    "                    stride=2\n",
    "                )\n",
    "            )\n",
    "            # Second append are the TWO CONVS, the in channels has to be double, refer to u_net diagram\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "        \n",
    "        # This is the horizontal path between downward and upward, features[-1] = 512\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        # This is the very last conv, from 388,388,64 to 388,388 or as in the paper: 388,388,2. It does not change the size of the image, it only changes the channels.\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Stores the outputs that will skip into the upward path\n",
    "        skip_connections = []\n",
    "        \n",
    "        # All the down layers are store in self.downs because we did it in __init__, so now, just run trough them\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        # Simply reversing the list, because of the upward path will use it in inverse order\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        # Step=2 because the upward path has a UP and a DoubleConv, but the skip only applies to the UP part.\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            # Integer division by 2 because, altough we want to skip the DoubleConv, we also want to run through the skip_connections one by one.\n",
    "            # Notice that we are not overwriting skip_connectionS <-- plural\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            ''' The INPUT needs to be shaped on a multiple of 16, since it is four down ways. If that is not the case, there will be an error to concatenate because of the MAXPOOL, since them both need same height and width.\n",
    "            One work around this is to check if they are different and resize the X '''\n",
    "            if x.shape != skip_connection.shape:\n",
    "                # Shape has: 0 BATCH_SIZE, 1 N_CHANNELS, 2 HEIGHT, 3 WIDTH. With [2:] we are taking only height and width.\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            # We have 4 dims, 0 BATCH, 1 CHANNEL, 2 HEIGHT, 3 WIDTH. We are concatenating them along the channel dimension\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            # This will do the DoubleConv after we did the UP and concatenated the skip connection\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        \n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P0DAk97hv8UR",
   "metadata": {
    "id": "P0DAk97hv8UR"
   },
   "source": [
    "## Sanity Check on the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FKa5QhGfv269",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKa5QhGfv269",
    "outputId": "5da48ca1-b6d0-4708-f340-54c4dafda888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 160, 160])\n",
      "torch.Size([3, 1, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    x = torch.randn((3, 1, 160, 160))\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "    print(x.shape)\n",
    "\n",
    "    assert preds.shape == x.shape\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3c64c",
   "metadata": {
    "id": "0ec3c64c"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91217028",
   "metadata": {
    "id": "91217028"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "# import utils as Utils\n",
    "\n",
    "from IPython.display import clear_output \n",
    "\n",
    "\n",
    "class Train():\n",
    "\n",
    "    def __init__(self, train_dir, train_maskdir, val_dir, val_maskdir, batch_size, n_epochs, n_workers, learning_rate, img_height, img_width, device, model, loss_fn):\n",
    "        self.train_dir = train_dir\n",
    "        self.train_maskdir = train_maskdir\n",
    "        self.val_dir = val_dir\n",
    "        self.val_maskdir = val_maskdir\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_workers = n_workers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.device = device\n",
    "        self.model = model.to(device=self.device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lists = []\n",
    "        self.optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.learning_rate\n",
    "                )\n",
    "\n",
    "        # TODO: If needed change Normalize mean to 0.0, and std to 1.0\n",
    "        self.t_transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_height, self.img_width)),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(\n",
    "            #     (0.5, 0.5, 0.5), \n",
    "            #     (0.5, 0.5, 0.5)\n",
    "            #     ),\n",
    "            ])\n",
    "        self.m_transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_height, self.img_width)),\n",
    "            transforms.ToTensor(),\n",
    "            ])\n",
    "\n",
    "        self.train_loader, self.v_loader = get_loaders(\n",
    "                train_dir=self.train_dir,\n",
    "                train_maskdir=self.train_maskdir,\n",
    "                val_dir=self.val_dir,\n",
    "                val_maskdir=self.val_maskdir,\n",
    "                batch_size=self.batch_size,\n",
    "                train_transform=self.t_transform,\n",
    "                mask_transform=self.m_transform,\n",
    "                num_workers=self.n_workers,\n",
    "                pin_memory=True)\n",
    "\n",
    "\n",
    "    def train_one_epoch(self, epoch_index):\n",
    "        running_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            # clear_output()\n",
    "            \n",
    "            # print('Train this epoch, train_loader index:', i, '|', len(self.train_loader))\n",
    "            inputs, labels = data\n",
    "\n",
    "            # TODO: This needs to be revised, it was done to fix the input shape into the expected one\n",
    "            # inputs = inputs.permute(0, 3, 1, 2)\n",
    "            # labels = labels.unsqueeze(1)\n",
    "            # print('shape:', inputs.shape, labels.shape)\n",
    "\n",
    "            inputs = inputs.float().to(self.device)\n",
    "            labels = labels.float().to(self.device) # Just making sure\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "              outputs = self.model(inputs)\n",
    "              loss = self.loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i%10 == 9:\n",
    "                last_loss = running_loss/10\n",
    "                print('\\tBatch {} Loss: {}'.format(i+1, last_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # print('1.', Utils.get_gpu_mem())\n",
    "            del(inputs)\n",
    "            del(labels)\n",
    "            torch.cuda.empty_cache()\n",
    "            # print('2.', Utils.get_gpu_mem())\n",
    "\n",
    "        return last_loss\n",
    "\n",
    "\n",
    "    # Video method\n",
    "    def train_one_epoch(self, epoch_index):\n",
    "        running_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "\n",
    "        # data = next(iter(self.train_loader))\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            # clear_output()\n",
    "            \n",
    "            # print('Train this epoch, train_loader index:', i, '|', len(self.train_loader))\n",
    "            inputs, labels = data\n",
    "\n",
    "            # TODO: This needs to be revised, it was done to fix the input shape into the expected one\n",
    "            # inputs = inputs.permute(0, 3, 1, 2)\n",
    "            # labels = labels.unsqueeze(1)\n",
    "            # print('shape:', inputs.shape, labels.shape)\n",
    "\n",
    "            inputs = inputs.float().to(self.device)\n",
    "            labels = labels.float().to(self.device) # Just making sure\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "              outputs = self.model(inputs)\n",
    "              loss = self.loss_fn(outputs, labels)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i%10 == 9 or i == 0:\n",
    "                last_loss = running_loss/10\n",
    "                print('\\tBatch {}|{} Loss: {}'.format(i+1, len(self.train_loader), last_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # print('1.', Utils.get_gpu_mem())\n",
    "            del(inputs)\n",
    "            del(labels)\n",
    "            torch.cuda.empty_cache()\n",
    "            # print('2.', Utils.get_gpu_mem())\n",
    "\n",
    "        return last_loss\n",
    "\n",
    "\n",
    "    def training(self):\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        epoch_number = 0\n",
    "        best_vloss = 1_000_000.0\n",
    "        best_dice = 0.0\n",
    "        best_dice_ep = -1\n",
    "\n",
    "        # early stop\n",
    "        last_loss = 100\n",
    "        patience = 3\n",
    "        trigger_times = 0\n",
    "        delta = 0.001\n",
    "\n",
    "        # plotting\n",
    "        v_dice_list = []\n",
    "        dice_list = []\n",
    "        loss = []\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            print('Epoch: {}|{}'.format(epoch+1, self.n_epochs))\n",
    "            self.model.train(True)\n",
    "            avg_loss = self.train_one_epoch(epoch)\n",
    "            self.model.train(False)\n",
    "\n",
    "            # print('Validation for', epoch+1)\n",
    "            # running_vloss = 0.0\n",
    "            # i = 0\n",
    "            # for i,v_data in enumerate(self.v_loader):\n",
    "            #     v_inputs, v_labels = v_data\n",
    "            #     # print('Input:', v_inputs.shape, 'label:', v_labels.shape)\n",
    "\n",
    "            #     v_inputs = v_inputs.float().to(self.device)\n",
    "            #     v_labels = v_labels.float().to(self.device)\n",
    "            #     # print('Before dataloader:', Utils.get_gpu_mem())\n",
    "            #     # v_inputs = v_inputs.permute(0, 3, 1, 2).float().to(self.device)\n",
    "            #     # v_labels = v_labels.unsqueeze(1).float().to(self.device)\n",
    "            #     # print('After dataloader:', Utils.get_gpu_mem())\n",
    "\n",
    "            #     # print('Getting predictions')\n",
    "            #     torch.cuda.empty_cache()\n",
    "            #     # print('Before model:', Utils.get_gpu_mem())\n",
    "            #     # with torch.cuda.amp.autocast():\n",
    "            #     v_outputs = torch.sigmoid(self.model(v_inputs))\n",
    "\n",
    "            #     # print('After model:', Utils.get_gpu_mem())\n",
    "            #     # print(v_outputs.shape, v_labels.shape)\n",
    "            #     v_loss = self.loss_fn(v_outputs, v_labels)                \n",
    "\n",
    "            #     # v_outputs = v_outputs * 255\n",
    "            #     # print('\\tInput shape', v_inputs.shape)\n",
    "            #     # print('\\tInput Min Max', v_inputs.min(), v_inputs.max())\n",
    "            #     # print('\\tOutput Min Max', v_outputs.min(), v_outputs.max())\n",
    "            #     # print('\\tOutput shape', v_outputs.shape)\n",
    "            #     # torchvision.utils.save_image(v_outputs, 'saved_images/v_output_epoch_{:03d}.png'.format(epoch+1))\n",
    "\n",
    "            #     # print('3.', Utils.get_gpu_mem())\n",
    "            #     del(v_inputs)\n",
    "            #     del(v_labels)\n",
    "            #     torch.cuda.empty_cache()\n",
    "            #     # print('4.', Utils.get_gpu_mem())\n",
    "\n",
    "            #     running_vloss += v_loss\n",
    "\n",
    "            # # print('Calculating, average loss')\n",
    "            # avg_vloss = running_vloss / (i+1)\n",
    "            # print('Loss Train: {}\\t Validation: {}'.format(avg_loss, avg_vloss))\n",
    "            # print('---TRAINING---')\n",
    "            dice, acc = check_accuracy(self.train_loader, self.model, device=self.device)\n",
    "            # print('----------------')\n",
    "\n",
    "            # print('---VALIDATION---')\n",
    "            v_dice, v_acc = check_accuracy(self.v_loader, self.model, device=self.device)\n",
    "            # print('----------------')\n",
    "            print('\\tdice: {:.5f} acc: {:.2f}\\t v_dice: {:.5f} v_acc: {:.2f} E_S: {}'.format(dice, acc, v_dice, v_acc, trigger_times))\n",
    "\n",
    "            # Early stopping\n",
    "            if v_dice - delta <= last_loss:\n",
    "              trigger_times += 1\n",
    "\n",
    "              if trigger_times >= patience and epoch > 30:\n",
    "                print('\\n---Early Stop trigger---\\n{} Epoch\\t Dice: {}'.format(epoch+1, v_dice))\n",
    "                return self.model\n",
    "\n",
    "            else:\n",
    "              trigger_times = 0\n",
    "\n",
    "            last_loss = v_dice\n",
    "            # END Early stopping\n",
    "\n",
    "            # plotting\n",
    "            loss.append(avg_loss)\n",
    "            v_dice_list.append(v_dice)\n",
    "            dice_list.append(dice)\n",
    "            # END plotting\n",
    "\n",
    "            # if dice > best_dice:\n",
    "            #   best_dice = dice\n",
    "            #   best_dice_ep = epoch_number\n",
    "\n",
    "            # if avg_vloss < best_vloss:\n",
    "            #     best_vloss = avg_vloss\n",
    "            #     model_path = 'model_{}_{:05d}'.format(timestamp, epoch_number)\n",
    "            #     torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "            # print('Best dice so far: {}, epoch: {}'.format(best_dice, best_dice_ep))\n",
    "\n",
    "        self.lists.append(loss)\n",
    "        self.lists.append(dice_list)\n",
    "        self.lists.append(v_dice_list)\n",
    "\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d52b32",
   "metadata": {
    "id": "79d52b32"
   },
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3596e44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3596e44",
    "outputId": "425e378e-5e50-4568-f53a-bde6b9e90fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffix: /content/drive/MyDrive/\n",
      "Instantiating U-Net Traning!\n"
     ]
    }
   ],
   "source": [
    "local = False\n",
    "path_suffix = None\n",
    "\n",
    "if local:\n",
    "    path_suffix = '/home/leite/Drive/'\n",
    "else:\n",
    "    path_suffix = '/content/drive/MyDrive/'\n",
    "\n",
    "print('Suffix:', path_suffix)\n",
    "\n",
    "train_dir = path_suffix + 'db/segmentation/mini-carvana/train/images'\n",
    "train_maskdir = path_suffix + 'db/segmentation/mini-carvana/train/masks/'\n",
    "val_dir = path_suffix + 'db/segmentation/mini-carvana/val/images/'\n",
    "val_maskdir = path_suffix + 'db/segmentation/mini-carvana/val/masks/'\n",
    "\n",
    "# This eliminates the need of SIGMOID\n",
    "l_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print('Instantiating U-Net Traning!')\n",
    "# TODO: Define optimizer out here\n",
    "unet_train = Train(\n",
    "        train_dir=train_dir,\n",
    "        train_maskdir=train_maskdir,\n",
    "        val_dir=val_dir,\n",
    "        val_maskdir=val_maskdir,\n",
    "        batch_size=32,\n",
    "        n_epochs=1000,\n",
    "        n_workers=2,\n",
    "        learning_rate=1e-4,\n",
    "        img_height=160,\n",
    "        img_width=240,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model=UNET(in_channels=3, out_channels=1),\n",
    "        loss_fn=l_func\n",
    "        )\n",
    "\n",
    "# summary(unet_train, (3, 160, 240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xVRuNSAGMdrA",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVRuNSAGMdrA",
    "outputId": "41a0b133-f1ea-4d4a-a506-dfb363328d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1|1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBatch 1|12 Loss: 0.0741932213306427\n",
      "\tBatch 10|12 Loss: 0.4972969740629196\n",
      "\tdice: 0.00000 acc: 77.67\t v_dice: 0.00000 v_acc: 78.00 E_S: 0\n",
      "Epoch: 2|1000\n",
      "\tBatch 1|12 Loss: 0.0418694943189621\n",
      "\tBatch 10|12 Loss: 0.32074037194252014\n",
      "\tdice: 0.00007 acc: 77.67\t v_dice: 0.00000 v_acc: 78.00 E_S: 1\n",
      "Epoch: 3|1000\n",
      "\tBatch 1|12 Loss: 0.03099939227104187\n"
     ]
    }
   ],
   "source": [
    "# print('Training U-Net... with', unet_train.device)\n",
    "trained_model = unet_train.training()\n",
    "\n",
    "# trained_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    # in_channels=3, out_channels=1, init_features=32, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb2bed",
   "metadata": {
    "id": "0ddb2bed",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save_predictions_as_imgs(unet_train.v_loader, unet_train.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hFMHodYy80bC",
   "metadata": {
    "id": "hFMHodYy80bC"
   },
   "outputs": [],
   "source": [
    "# plot_predictions(unet_train.v_loader)\n",
    "\n",
    "i_loader = iter(unet_train.v_loader)\n",
    "img, lbl = i_loader.next()\n",
    "\n",
    "print('Infos:')\n",
    "print('\\tType:', type(img[0]), type(lbl[0]))\n",
    "print('\\tShape:', img[0].shape, lbl[0].shape)\n",
    "print('\\tMax:', torch.max(img[0]), torch.max(lbl[0]))\n",
    "fig = img[0].numpy()\n",
    "print('\\tMax:', fig.max())\n",
    "fig = (fig*255).astype(int)\n",
    "print('\\tMax:', fig.max())\n",
    "\n",
    "render_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2IIkjmZhEVYG",
   "metadata": {
    "id": "2IIkjmZhEVYG"
   },
   "outputs": [],
   "source": [
    "plot_lists(unet_train.lists[1], unet_train.lists[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0lsR_ZArE7dZ",
   "metadata": {
    "id": "0lsR_ZArE7dZ"
   },
   "outputs": [],
   "source": [
    "plot_lists(unet_train.lists[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "u-net.sync.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
