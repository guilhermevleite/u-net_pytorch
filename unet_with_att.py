# -*- coding: utf-8 -*-
"""UNET-with_att.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1maij6TTO8w-0uu0gR7i_XNM8X86aaUnq
"""

from google.colab import drive
drive.mount('/content/drive')

# coding: utf-8
'''
Dependencies:
    Keras 2.0.8
    Tensorflow 1.3.0
    Config
Usage:
from keras.optimizers import Adam
model = Attention_ResUNet()
model.load_weights(weights_filename) # optional
optim = Adam() # optimizer
loss = dice_coef_loss # loss function
metrics = [dice_coef]
model.compile(optimizer=optim, loss=dice_coef_loss, metrics=[dice_coef]) # configuration
model.fit(...)
'''
__author__ = 'MoleImg'
import tensorflow as tf
from tensorflow.keras import models, layers, regularizers
from tensorflow.keras import backend as K

#import Config as conf

'''
Hyper-parameters
'''
# input data
INPUT_SIZE = 256
INPUT_CHANNEL = 1   # 1-grayscale, 3-RGB scale
OUTPUT_MASK_CHANNEL = 1
# network structure
FILTER_NUM = 32 # number of basic filters for the first layer
FILTER_SIZE = 3 # size of the convolutional filter
DOWN_SAMP_SIZE = 2 # size of pooling filters
UP_SAMP_SIZE =2 # size of upsampling filters

'''
Definitions of loss and evaluation metrices
'''

def jacard_coef(y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)


def jacard_coef_loss(y_true, y_pred):
    return -jacard_coef(y_true, y_pred)



def expend_as(tensor, rep):
     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),
                          arguments={'repnum': rep})(tensor)


def double_conv_layer(x, filter_size, size, dropout, batch_norm=False):
    '''
    construction of a double convolutional layer using
    SAME padding
    RELU nonlinear activation function
    :param x: input
    :param filter_size: size of convolutional filter
    :param size: number of filters
    :param dropout: FLAG & RATE of dropout.
            if < 0 dropout cancelled, if > 0 set as the rate
    :param batch_norm: flag of if batch_norm used,
            if True batch normalization
    :return: output of a double convolutional layer
    '''
    axis = 3
    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)
    if batch_norm is True:
        conv = layers.BatchNormalization()(conv)
    conv = layers.Activation('relu')(conv)
    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)
    if batch_norm is True:
        conv = layers.BatchNormalization()(conv)
    conv = layers.Activation('relu')(conv)
    if dropout > 0:
        conv = layers.Dropout(dropout)(conv)

    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)
    if batch_norm is True:
        shortcut = layers.BatchNormalization()(shortcut)

    res_path = layers.add([shortcut, conv])
    return res_path

def gating_signal(input, out_size, batch_norm=False):
    """
    resize the down layer feature map into the same dimension as the up layer feature map
    using 1x1 conv
    :param input:   down-dim feature map
    :param out_size:output channel number
    :return: the gating feature map with the same dimension of the up layer feature map
    """
    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)
    if batch_norm:
        x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    return x

def attention_block(x, gating, inter_shape):
    shape_x = K.int_shape(x)
    shape_g = K.int_shape(gating)

    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16
    shape_theta_x = K.int_shape(theta_x)

    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)
    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),
                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),
                                 padding='same')(phi_g)  # 16

    concat_xg = layers.add([upsample_g, theta_x])
    act_xg = layers.Activation('relu')(concat_xg)
    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)
    sigmoid_xg = layers.Activation('sigmoid')(psi)
    shape_sigmoid = K.int_shape(sigmoid_xg)
    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32

    upsample_psi = expend_as(upsample_psi, shape_x[3])

    y = layers.multiply([upsample_psi, x])

    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)
    result_bn = layers.BatchNormalization()(result)
    return result_bn


def Attention_ResUNet(dropout_rate=0.0, batch_norm=True):
    '''
    Rsidual UNet construction, with attention gate
    convolution: 3*3 SAME padding
    pooling: 2*2 VALID padding
    upsampling: 3*3 VALID padding
    final convolution: 1*1
    :param dropout_rate: FLAG & RATE of dropout.
            if < 0 dropout cancelled, if > 0 set as the rate
    :param batch_norm: flag of if batch_norm used,
            if True batch normalization
    :return: model
    '''
    # input data
    # dimension of the image depth
    inputs = layers.Input((INPUT_SIZE, INPUT_SIZE, INPUT_CHANNEL), dtype=tf.float32)
    axis = 3

    # Downsampling layers
    # DownRes 1, double residual convolution + pooling
    conv_128 = double_conv_layer(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)
    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)
    # DownRes 2
    conv_64 = double_conv_layer(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)
    # DownRes 3
    conv_32 = double_conv_layer(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)
    # DownRes 4
    conv_16 = double_conv_layer(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)
    # DownRes 5, convolution only
    conv_8 = double_conv_layer(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)

    # Upsampling layers
    # UpRes 6, attention gated concatenation + upsampling + double residual convolution
    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)
    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)
    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(conv_8)
    up_16 = layers.concatenate([up_16, att_16], axis=axis)
    up_conv_16 = double_conv_layer(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 7
    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)
    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)
    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_16)
    up_32 = layers.concatenate([up_32, att_32], axis=axis)
    up_conv_32 = double_conv_layer(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 8
    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)
    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)
    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_32)
    up_64 = layers.concatenate([up_64, att_64], axis=axis)
    up_conv_64 = double_conv_layer(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)
    # UpRes 9
    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)
    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)
    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format="channels_last")(up_conv_64)
    up_128 = layers.concatenate([up_128, att_128], axis=axis)
    up_conv_128 = double_conv_layer(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)

    # 1*1 convolutional layers
    # valid padding
    # batch normalization
    # sigmoid nonlinear activation
    conv_final = layers.Conv2D(OUTPUT_MASK_CHANNEL, kernel_size=(1,1))(up_conv_128)
    conv_final = layers.BatchNormalization(axis=axis)(conv_final)
    conv_final = layers.Activation('relu')(conv_final)

    # Model integration
    model = models.Model(inputs, conv_final, name="AttentionResUNet")
    return model

pip install keras-rectified-adam

import numpy as np
from tensorflow.keras.utils import Sequence
import cv2
from keras import backend as K
import os
from tensorflow.keras import regularizers
from keras_radam import RAdam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

from tensorflow.keras.utils import Sequence

class DataGenerator(Sequence):
    'Generates data for Keras'
    
    def __init__(self, batch_size, dim, input_img_paths):
        self.batch_size = batch_size
        self.dim = dim
        self.input_img_paths = input_img_paths
        self.shuffle = False
        self.n_channels = 1
        self.on_epoch_end()
       

    def __len__(self):
        'Denotes the number of batches per epoch'
        return len(self.input_img_paths) // self.batch_size

    def __getitem__(self, idx):
        'Generate one batch of data'
        # Generate indexes of the batch
        i = idx * self.batch_size  

        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]
        #batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]

        # Generate data
        X = self._generate_X(batch_input_img_paths)
        y = self._generate_y(batch_input_img_paths)
        
        return X,y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        pass

    # Normalizes values of a matrix between -1 and 1
    def myNormalization(self, data):
        max_val = np.max(data)
        min_val = np.min(data)
        
        return  2 * (data[:,:] - min_val) / (max_val - min_val) - 1


    def _load_input_image(self, image_path):
        'Load image normalized'
        img = np.load(image_path)
        img = self.myNormalization(img)
        img = cv2.resize(img.astype('float32'), (256,256), interpolation=cv2.INTER_LINEAR)
        return img

    def _load_target_image(self, image_path):
        'Load target image'
        img = np.load(image_path)
        img = cv2.resize(img.astype('float32'), (256,256), interpolation=cv2.INTER_LINEAR)
        kernel = np.ones((4,4),np.uint8)
        img = np.uint8(img)
        img = cv2.dilate(img,kernel, iterations = 3)
        kernel_e = np.ones((3,3),np.uint8)
        img = cv2.erode(img,kernel_e, iterations = 3)
        #img = np.reshape(img, (256,256,1))

        return img

    def _generate_X(self, batch_input_img_paths):
        'Generates data containing batch_size images'
        # Initialization
        X = np.empty((self.batch_size, *self.dim))
        
        # Generate data
        for i, path in enumerate(batch_input_img_paths):
            # Store sample
            aux =  self._load_input_image(path)
            X[i,] = np.reshape( aux,  (256,256,1) )
            
        return X
    
    def _generate_y(self, batch_input_img_paths):
        'Generates data containing batch_size masks'
        y =  np.empty((self.batch_size, *self.dim))
        
        # Generate data
        for i, path in enumerate(batch_input_img_paths):
            # Store sample
            y[i,] = np.reshape(self._load_target_image(path.replace('seismic', 'fault').replace('input','target')) , (256,256,1))
            
        return y

from numpy.random import seed
#seed(12345)
import tensorflow as tf
#tf.random.set_seed(1234)
import os
import random
import numpy as np
import skimage
import matplotlib.pyplot as plt

def dice_coef(y_true, y_pred, smooth=1):
  intersection = K.sum(y_true * y_pred, axis=[1,2,3])
  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])
  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)
  return dice

def showHistory(history):
  # list all data in history
  print(history.history.keys())
  fig = plt.figure(figsize=(10,6))

  # summarize history for accuracy
  plt.plot(history.history['dice_coef'])
  plt.plot(history.history['val_dice_coef'])
  plt.title('Model accuracy',fontsize=20)
  plt.ylabel('dice_coef',fontsize=20)
  plt.xlabel('Epoch',fontsize=20)
  plt.legend(['train', 'val'], loc='center right',fontsize=20)
  plt.tick_params(axis='both', which='major', labelsize=18)
  plt.tick_params(axis='both', which='minor', labelsize=18)
  plt.show()

  # summarize history for loss
  fig = plt.figure(figsize=(10,6))
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model loss',fontsize=20)
  plt.ylabel('Loss',fontsize=20)
  plt.xlabel('Epoch',fontsize=20)
  plt.legend(['train', 'val'], loc='center right',fontsize=20)
  plt.tick_params(axis='both', which='major', labelsize=18)
  plt.tick_params(axis='both', which='minor', labelsize=18)
  plt.show()

root_dir = "/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/data_postprocessing"
currentCls = '/input'
src = root_dir+currentCls

train_FileNames = np.load(root_dir+'/names_train.npy')
val_FileNames = np.load(root_dir+'/names_val.npy')

def goTrain(): 
  
   
  #data load first
  input_img_paths_train =  train_FileNames
  input_img_paths_val = val_FileNames

  
  batch_size = 32
  image_size = (256,256,1)

  training_generator = DataGenerator(batch_size, image_size, input_img_paths_train)
  validation_generator = DataGenerator(batch_size, image_size, input_img_paths_val)
  
  #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)
  model = Attention_ResUNet()
  
  custom_early_stopping = EarlyStopping(
    monitor='val_dice_coef', 
    patience=20, 
    min_delta=0.001, 
    mode='max'
  )

  model.compile(RAdam(lr=1e-4), loss = 'binary_crossentropy',  metrics=[dice_coef])
  # Calculate the weights for each class so that we can balance the data
  
  model.summary()

  # checkpoint
  filepath="/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/check-unet-inter/fseg-dados_selecionados-dice/AUnettable-posprocessing-alldata-epoc+{epoch:02d}.hdf5"
  checkpoint = ModelCheckpoint(filepath, monitor='val_dice_coef', 
        verbose=1, save_best_only=True, mode='max')
  
  print("data prepared, ready to train!")
  #model.load_weights('/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/check-unet-inter/fseg-dados_selecionados-dice/AUnet-table-ES-4kt-1kv-200epoc.hdf5')
 
  # Fit the model
  history_model = model.fit_generator(generator=training_generator,
    validation_data=validation_generator,callbacks=[checkpoint, custom_early_stopping], epochs=250)
  model.save('/content/drive/MyDrive/Pesquisa-Doc/Colab Notebooks/check-unet-inter/fseg-dados_selecionados-dice/AUnet-table-posprocessing-alldata+100epoc.hdf5')

  showHistory(history_model)

def main():
  goTrain()

main()